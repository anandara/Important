--------------------------------
1. Data For Task
---------------------------------
hadoopexam1.txt
I am learning Apache Spark from HadoopExam Learning Resources
I am learning Apache Hadoop from HadoopExam Learning Resources
I have created my technical profile at www.QuickTechie.com
I am learning Apache Spark from Training4exam Learning Resources

Accomplish the followings:-

1. Create this text file in HDFS
2. Once file is created, write the spark application which will read
from HDFS as an RDD
3. Once RDD loaded, do the line count of this RDD

---------------------------------
2. Data For Task
---------------------------------
hadoopexam2.txt
I am learning Apache Spark from HadoopExam Learning Resources
I am learning Apache Hadoop from HadoopExam Learning Resources
I have created my technical profile at www.QuickTechie.com
I am learning Apache Spark from Training4exam Learning Resources
I am learning Apache Spark from Training4exam Learning Resources

Accomplish the followings:-
1. Create this file in HDFS
2. Once file is created, write a spark application which will read
this file from HDFS as an RDD
3. Filter all records contains 'HadoopExam' in line and count of lines
4. Filter all records does not contains 'HadoopExam' in line and count the lines

---------------------------------
3. Data For Task
---------------------------------
("We", "Are" ,"Learning" , "Hadoop" , "From" , "HadoopExam" , "We",
"Are" ,"Learning" , "Spark" , "From" , "HadoopExam.com" , "hadoop" ,
"HADOOP")

Accomplish the followings:-

1. Create an RDD using using the given words
2. Once RDD is created count all the words
3. Now filter out all the words which does not have Hadoop keyword,
however make sure it would count all the different cases(upper/lower)
as well

---------------------------------
4. Data For Task
---------------------------------
hdpcd/hadoopexam4A.txt
HadoopExam.com QuickTechie.com Training4Exam.com


hdpcd/hadoopexam4B.txt
Hadoop Spark Scala Python Java Cloud Science

hdpcd/hadoopexam4C.txt
India USA UK Canada Australia

Accomplish the followings:-

1. Load all 3 files in different RDDS
2. Concatnate all the data in single RDDS
3. Count all the words in of all 3 files

---------------------------------
5. Data For Task
---------------------------------
hdpcd/dir5A/hadoopexamA.txt
HadoopExam.com QuickTechie.com Training4Exam.com

hdpcd/dir5B/hadoopexamB.txt
Hadoop Spark Scala Python Java Cloud Science

hdpcd/dir5C/hadoopexamC.txt
India USA UK Canada Australia

Accomplish the followings:-

1. Load the data given in all 3 directories
2. Once the data is loaded filter the words which contains "Hadoop"
3. Now persist the data in memory

---------------------------------
6. Data For Task
---------------------------------
You have given the follwing data in a file hadoophortonworks6.txt
CourseName,Price,TaxandOthersInPercen
Hadoop,2900,15
Spark,3500,14
AWS,2700,13
Azure,2800,11
Java,3000,16
HBase,3200,20

Accomplish the followings:-

1. Load this csv file in RDD
2. Now calculate the final price using tax
3. Save the final data in HDFS

---------------------------------
7. Data For Task
---------------------------------
You have given 3 csv files hdfs as below:-

EmployeeManager.csv
E01,Vishnu
E02,Satyam
E03,Shiv
E04,Sundar
E05,John
E06,Pallavi
E07,Tanvir
E08,Shekhar
E09,Vinod
E10,Jitendra

EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat

EmployeeSalary.csv
E01,50000
E02,50000
E03,45000
E04,45000
E05,50000
E06,45000
E07,50000
E08,10000
E09,10000
E10,10000

Accomplish the following:-

Using spark and its API you have to generate a joined output as below
and save as text file (Separated by comma)
for final distribution and output must be sorted by id
id,name,salary,managerName

---------------------------------
8. Data For Task
---------------------------------
You have given following 2 files:-

Content.txt
Hello this is HadoopExam.com
This is QuickTechie.com
Apache Spark Training
This is Spark Learning Session
Spark is faster than MapReduce

Remove.txt
Hello, is, this, the

Accomplish the following:-

write the spark program which reads the content.txt file and load as
RDD, remove all the words from the broadcast variables
(which is loaded as an RDDs of words from Remove.txt). And count the
occurances of each word and save it as text file in HDFS

---------------------------------
9. Data For Task
---------------------------------
You have given following 3 files as below:-

spark3/sparkdir1/file1.txt
Apache Hadoop is an open-source software framework written in Java for
distributed storage and distributed processing of very large data sets
on computer clusters built from commodity hardware. All the modules in
Hadoop are designed with a fundamental assumption that hardware
failures are common and should be automatically handled by the
framework

spark3/sparkdir2/file2.txt
The core of Apache Hadoop consists of a storage part known as Hadoop
Distributed File System (HDFS) and a processing part called MapReduce.
Hadoop splits files into large blocks and distributes them across
nodes in a cluster. To process data, Hadoop transfers packaged code
for nodes to process in parallel based on the data that needs to be
processed.

spark3/sparkdir3/file3.txt
his approach takes advantage of data locality nodes manipulating the
data they have access to to allow the dataset to be processed faster
and more efficiently than it would be in a more conventional
supercomputer architecture that relies on a parallel file system where
computation and data are distributed via high-speed networking

Accomplish the following:-

write the spark program which loads the all 3 file from hdfs and do
the word count by filtering the following words
And result should be sorted by word count in reverse order

Filter words("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of")

Also please make sure you load all 3 files as Single RDD(All 3 files
must be loaded using single API call)

You have also been given following codec
import org.apache.hadoop.io.compress.GzipCodec

Please use above codec to compress file, while saving in HDFS

---------------------------------
10. Data For Task
---------------------------------
You have given files as below:-

EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat

EmployeeSalary.csv
E01,50000
E02,50000
E03,45000
E04,45000
E05,50000
E06,45000
E07,50000
E08,10000
E09,10000
E10,10000

Accomplish the following:-

Now write a spark code in scala which will load these two files from
HDFS and join the same, and produce the (name,salary) values. And save
the data in multiple file group by salary(Means each file will have
name of employes with same salary). Make sure file name include salary
as well.
